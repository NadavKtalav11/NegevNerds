{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the Backend directory dynamically\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(current_dir, '../..')))\n",
    "\n",
    "# Import the required modules\n",
    "from BusinessLayer.User.User import User\n",
    "from BusinessLayer.User.UserFacade import *\n",
    "\n",
    "from BusinessLayer.Util.Exceptions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_mail = \"david.volodarski1@gmail.com\"\n",
    "valid_mail = \"volodavi@post.bgu.ac.il\"\n",
    "valid_mail1 = \"puzis@bgu.ac.il\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_controller = UserFacade()\n",
    "len(user_controller.users_byEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_controller.register(valid_mail, \"12a345D{\", \"david\", \"volodarsky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the main page containing the iframe\n",
    "main_url =  \"https://in.bgu.ac.il/engn/iem/Pages/CoursesList1.aspx\"\n",
    "\n",
    "\n",
    "# Fetch the main page\n",
    "response = requests.get(main_url)\n",
    "response.raise_for_status()\n",
    "main_html = response.text\n",
    "\n",
    "# Parse the main page\n",
    "main_soup = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "# Find the iframe and extract its 'src' attribute\n",
    "iframe = main_soup.find(\"iframe\")\n",
    "iframe_url = iframe[\"src\"]\n",
    "\n",
    "# Full URL of the iframe content (handle relative URLs)\n",
    "iframe_url = requests.compat.urljoin(main_url, iframe_url)\n",
    "\n",
    "# Fetch the iframe content\n",
    "iframe_response = requests.get(iframe_url)\n",
    "iframe_response.raise_for_status()\n",
    "iframe_html = iframe_response.text\n",
    "\n",
    "# Parse the iframe content\n",
    "iframe_soup = BeautifulSoup(iframe_html, \"html.parser\")\n",
    "\n",
    "# Locate the table within the iframe content\n",
    "table = iframe_soup.find(\"table\")  # Adjust this based on actual structure\n",
    "print(table)\n",
    "rows = table.find_all(\"tr\") if table else []\n",
    "\n",
    "# Extract the table data\n",
    "courses = {}\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    if len(cells) == 3:  # Assuming 3 columns: course_id, active_semester, course_name\n",
    "        course_id = cells[0].text.strip()\n",
    "        active_semester = cells[1].text.strip()\n",
    "        course_name = cells[2].text.strip()\n",
    "        courses[course_id] = {\n",
    "            \"active_semester\": active_semester,\n",
    "            \"course_name\": course_name\n",
    "        }\n",
    "\n",
    "# Print or process the extracted courses\n",
    "print(courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the iframe or webpage containing the table\n",
    "url = \"https://in.bgu.ac.il/engn/iem/Pages/CoursesList1.aspx\"\n",
    "\n",
    "# Fetch the HTML content\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "print(soup)\n",
    "# Find the rows in the table\n",
    "rows = soup.find_all(\"tr\")\n",
    "\n",
    "# Extract data from each row\n",
    "courses = {}\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    if len(cells) >= 3:  # Ensure it has enough columns\n",
    "        course_id = cells[0].text.strip()\n",
    "        active_semester = cells[1].text.strip()\n",
    "        course_name = cells[2].text.strip()\n",
    "        courses[course_id] = {\n",
    "            \"active_semester\": active_semester,\n",
    "            \"course_name\": course_name\n",
    "        }\n",
    "\n",
    "# Print or use the extracted courses dictionary\n",
    "print(courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "\n",
    "def extract_syllabus_topics4(file_path, topic_patterns):\n",
    "    \"\"\"\n",
    "    Extracts syllabus topics from a course PDF file and returns them as a set.\n",
    "    Handles diverse formats such as tables, bullet points, numbered sections, and headers.\n",
    "\n",
    "    :param file_path: Path to the PDF file\n",
    "    :param topic_patterns: List of regex patterns to identify syllabus-related sections\n",
    "    :return: A set of topics from the syllabus\n",
    "    \"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    syllabus_topics = set()\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # Match topics using provided patterns\n",
    "        for pattern in topic_patterns:\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "            for match in matches:\n",
    "                # Split potential topics by common delimiters and clean up\n",
    "                topics = re.split(r',|;|\\n|\\•|\\.', match)\n",
    "                syllabus_topics.update([topic.strip() for topic in topics if topic.strip()])\n",
    "\n",
    "        # Handle bullet points\n",
    "        lines = text.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if re.match(r'^\\•', line):  # Matches lines starting with \"•\"\n",
    "                syllabus_topics.add(line.lstrip(\"• \").strip())\n",
    "\n",
    "        # Handle numbered sections (e.g., \"1. Topic\", \"2. Topic\")\n",
    "        for line in lines:\n",
    "            if re.match(r'^\\d+\\.\\s', line):  # Matches lines starting with \"1. \", \"2. \", etc.\n",
    "                syllabus_topics.add(line.strip())\n",
    "\n",
    "        # Handle keywords directly in the text\n",
    "        if any(keyword in text for keyword in [\"סילבוס\", \"Topics\", \"Outline\"]):\n",
    "            for line in lines:\n",
    "                # Add lines containing relevant keywords as potential topics\n",
    "                if any(keyword in line for keyword in [\"סילבוס\", \"Topics\", \"Outline\"]):\n",
    "                    syllabus_topics.add(line.strip())\n",
    "\n",
    "    return syllabus_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Table 1\n",
      "Normalized Headers: ['קריאות', 'מועד פרסום', 'נושאי השיעור', 'פגישה']\n",
      "Matching column found: נושאי השיעור\n",
      "Matching Data:\n",
      "{'נורמליזציה  של  נתונים;  תלויות', 'אלגברה רלציונית', 'מיפוי ERD לסכמת בסיס נתונים רלציונית .', 'הנתונים', 'SQL שפת', 'מבוא  על  בסיסי  נתונים  ,DBMS,  סכמות', 'ER)(ותרשים  ;ERD', 'מבוא לתכנות באמצעות בסיסי נתונים', 'מודלים  סמנטיים;  מודל  ישויות - וקשרים', 'ומודלים .', 'פונקציונאליות  ומורכבות;  כללי  נרמול', 'מבוא ל עיבוד תנועות', 'וסינתזה;  תהליך  עיצוב  סכמת  בסיס', 'המודל הרלציוני', 'אופטימיזציה של שאילתות'}\n"
     ]
    }
   ],
   "source": [
    "from tabula import read_pdf\n",
    "import pandas as pd\n",
    "\n",
    "def extract_table_with_topics_final(pdf_path, topics, pages=\"all\"):\n",
    "    \"\"\"\n",
    "    Extracts tables from a PDF, matches column titles to a list of topics,\n",
    "    and returns data under matching columns.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file\n",
    "    :param topics: List of column titles to match\n",
    "    :param pages: Pages to extract tables from (default: \"all\")\n",
    "    :return: Set of data under matching columns\n",
    "    \"\"\"\n",
    "    matching_data = set()\n",
    "\n",
    "    try:\n",
    "        # Extract tables using Tabula\n",
    "        tables = read_pdf(pdf_path, pages=pages, multiple_tables=True, pandas_options={\"header\": None})\n",
    "\n",
    "        if not tables:\n",
    "            print(\"No tables found in the PDF.\")\n",
    "            return matching_data\n",
    "\n",
    "        for i, table in enumerate(tables):\n",
    "            print(f\"Processing Table {i + 1}\")\n",
    "\n",
    "            # Assume the first row is the header\n",
    "            df = pd.DataFrame(table)\n",
    "            headers = df.iloc[0]\n",
    "            df.columns = headers\n",
    "            df = df[1:]  # Remove the header row\n",
    "\n",
    "            # Clean up headers for matching\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            print(f\"Normalized Headers: {list(df.columns)}\")\n",
    "\n",
    "            # Check for matching columns\n",
    "            for column in df.columns:\n",
    "                if any(topic in column for topic in topics):\n",
    "                    print(f\"Matching column found: {column}\")\n",
    "                    matching_data.update(df[column].dropna().tolist())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during table extraction: {e}\")\n",
    "\n",
    "    return matching_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"/mnt/data/bsisi_netunim.pdf\"  # Path to your PDF file\n",
    "topics = [\"נושאי השיעור\", \"Topics\", \"Outline\"]  # List of column headers to search for\n",
    "file_path1 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\"\n",
    "\n",
    "# Extract data\n",
    "matching_data = extract_table_with_topics_final(file_path1, topics)\n",
    "\n",
    "# Print results\n",
    "print(\"Matching Data:\")\n",
    "print(matching_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def has_valid_table_with_pdfplumber(pdf_path, min_rows=2, min_columns=2):\n",
    "    \"\"\"\n",
    "    Checks if a PDF contains at least one valid table using pdfplumber.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file\n",
    "    :param min_rows: Minimum number of rows to validate a table\n",
    "    :param min_columns: Minimum number of columns to validate a table\n",
    "    :return: True if at least one valid table is found, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()  # Extract tables from the page\n",
    "                if tables:\n",
    "                    for table in tables:\n",
    "                        # Validate table structure\n",
    "                        if len(table) >= min_rows and len(table[0]) >= min_columns:\n",
    "                            print(f\"Valid table found in {os.path.basename(pdf_path)} on page {page_num + 1}\")\n",
    "                            return True\n",
    "        print(f\"No valid tables in {os.path.basename(pdf_path)}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(pdf_path)}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "def extract_syllabus_topics_with_pdfplumber(file_path, topic_patterns):\n",
    "    \"\"\"\n",
    "    Extracts syllabus topics from a course PDF file using pdfplumber and returns them as a set.\n",
    "    Handles diverse formats such as tables, bullet points, numbered sections, and headers.\n",
    "\n",
    "    :param file_path: Path to the PDF file\n",
    "    :param topic_patterns: List of regex patterns to identify syllabus-related sections\n",
    "    :return: A set of topics from the syllabus\n",
    "    \"\"\"\n",
    "    syllabus_topics = set()\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    # Match topics using provided patterns\n",
    "                    for pattern in topic_patterns:\n",
    "                        matches = re.findall(pattern, text, re.DOTALL)\n",
    "                        for match in matches:\n",
    "                            # Split potential topics by common delimiters and clean up\n",
    "                            topics = re.split(r',|;|\\n|\\•|\\.', match)\n",
    "                            syllabus_topics.update([topic.strip() for topic in topics if topic.strip()])\n",
    "\n",
    "                    # Handle bullet points\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    for line in lines:\n",
    "                        if re.match(r'^\\•', line):  # Matches lines starting with \"•\"\n",
    "                            syllabus_topics.add(line.lstrip(\"• \").strip())\n",
    "\n",
    "                    # Handle numbered sections (e.g., \"1. Topic\", \"2. Topic\")\n",
    "                    for line in lines:\n",
    "                        if re.match(r'^\\d+\\.\\s', line):  # Matches lines starting with \"1. \", \"2. \", etc.\n",
    "                            syllabus_topics.add(line.strip())\n",
    "\n",
    "                    # Handle keywords directly in the text\n",
    "                    if any(keyword in text for keyword in [\"סילבוס\", \"Topics\", \"Outline\"]):\n",
    "                        for line in lines:\n",
    "                            # Add lines containing relevant keywords as potential topics\n",
    "                            if any(keyword in line for keyword in [\"סילבוס\", \"Topics\", \"Outline\"]):\n",
    "                                syllabus_topics.add(line.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF with pdfplumber: {e}\")\n",
    "\n",
    "    return syllabus_topics\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_syllabus_topic_total(pdf_path):\n",
    "    topic_patterns = [\n",
    "    r'סילבוס[:\\n](.*?)\\n',  # Hebrew pattern for \"Syllabus\"\n",
    "    r'סילבוס באנגלית[:\\n](.*?)\\n',\n",
    "    r'סילבוס בעברית[:\\n](.*?)\\n',\n",
    "]\n",
    "    topic_patterns1 = [\n",
    "    r'נושאים[:\\n](.*?)\\n',  # Hebrew pattern for \"Topics\"\n",
    "    r'Course Topics[:\\n](.*?)\\n',\n",
    "    r'Outline[:\\n](.*?)\\n',\n",
    "]\n",
    "    topics_table = [\"נושאי השיעור\",\"נושא השיעור\", \"Topics\", \"Outline\"]  # List of column headers to search for\n",
    "\n",
    "    topics = set()\n",
    "    # print(topics)\n",
    "    pdf_copy = pdf_path\n",
    "    has_table = has_valid_table_with_pdfplumber(pdf_path)\n",
    "    if not has_table:\n",
    "        topics = extract_syllabus_topics_with_pdfplumber(pdf_copy,topic_patterns)\n",
    "        if len(topics)==0:\n",
    "            topics = extract_syllabus_topics_with_pdfplumber(pdf_path,topic_patterns1)\n",
    "    else:\n",
    "   \n",
    "         topics = extract_table_with_topics_final(pdf_path,topics_table )\n",
    "    #     topics = extract_syllabus_topic9(pdf_path)\n",
    "    # cleaned_topics = {topic.lstrip(\"• \").strip() for topic in topics}\n",
    "    cleaned_topics = set()\n",
    "    for topic in topics:\n",
    "        # Remove leading numbers (e.g., \"1.\", \"2. \", etc.)\n",
    "        topic = re.sub(r\"^\\d+\\.\\s*\", \"\", topic)\n",
    "        # Remove leading special characters like \"•\", \"*\", etc.\n",
    "        topic = topic.lstrip(\"•* \").strip()\n",
    "        if topic:  # Only keep non-empty topics\n",
    "            cleaned_topics.add(topic)\n",
    "    \n",
    "\n",
    "    return cleaned_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/data_strcutre.pdf\"  # Replace with the correct path to the PDF\n",
    "file_pdf1 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\"\n",
    "pdf_path3 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/histabrut.pdf\"\n",
    "pdf_4 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/compilatin.pdf\"\n",
    "pdf_5 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/decsionmaker.pdf\"\n",
    "pdf_6 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/mavo_miki.pdf\"\n",
    "pdf_7 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/dc_mavo.pdf\"\n",
    "pdf_8 =    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/modelim.pdf\" \n",
    "pdf_9 =     \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/eimut.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ב גנב ןוירוג-ןב תטיסרבינוא\n",
      "בשחמה יעדמל הקלחמה - עבטה יעדמל הטלוקפה\n",
      "ב\"פשת 'ב רטסמס\n",
      "סרוק סובליס\n",
      "ם ינותנ ינבמ : סרוק םש\n",
      "Data Structures :תילגנאב סרוק םש\n",
      "202-1-1031 :סרוק רפסמ\n",
      "הבוח :סרוק גוס\n",
      "5.0 :ז\"קנ\n",
      "שמש לכימ 'בג ,רוצ לקד 'פורפ ,ימרכ זפ 'פורפ :סרוקה הצרמ\n",
      ":םדק תושירד\n",
      "202-1-1011 – בשחמה יעדמל אובמ\n",
      ":תילגנאב סובליס\n",
      "• Growth of functions & algorithm analysis\n",
      "• Recurrences\n",
      "• Basic ADT’s: stacks & queues\n",
      "• Binary search trees\n",
      "• AVL-Trees\n",
      "• B-Trees\n",
      "• Probability basics\n",
      "• Skip lists\n",
      "• Hash tables\n",
      "• Bloom filter\n",
      "• Priority queues (heaps).\n",
      "• Compression: Huffman, Lempel-Ziv\n",
      "• Quicksort\n",
      "• Median (deterministic and randomized algorithms).\n",
      "• Sorting in Linear Time\n",
      "• Elementary graph algorithms: BFS, DFS, topological Sortב גנב ןוירוג-ןב תטיסרבינוא\n",
      "בשחמה יעדמל הקלחמה - עבטה יעדמל הטלוקפה\n",
      "ב\"פשת 'ב רטסמס\n",
      "• Amortized analysis\n",
      "• Data structures for disjoint sets (union find)\n",
      "• MST: Kruskal, Prim\n",
      ".סרוקה תינכותב םייוניש ונכתי\n",
      ":סרוקה אשונו תרטמ\n",
      "ןונכתלו בשחמב םינותנ ינבמב שומישו הרדגהל תיטקרטסבא תלוכי חתפל איה סרוקה תרטמ\n",
      "רידגנ ,בשחמה יעדמב תוידוסי תויעב ןורתפל םימתירוגלא םג דמלנ ךכ ךותב .םיליעי םימתירוגלא\n",
      "םימתירוגלא בלשלו ןנכתל דציכ דמלנ ,ןוסחא חפנו הציר ינמז לש ) תיטוטפמיסא( תוליעי בשחנו\n",
      ".תונוש ןכת תויעבל ומיאתיש ךכ םישדח םינותנ ינבמו\n",
      ":סרוקה ןויצ יביכרמו תושירד\n",
      "ןויצה בכרה ,רטסמסה ףוסב הניחב םייקל היהי ןתינו הדימב :הנורוקה תפגמ לשב ןוכדע\n",
      ".רהבתי בצמהשכ םסרופי יפוסה ןויצה בכרה ,תרחא .הטמל טרופמש יפכ היהי יפוסה\n",
      ".הניחבו םיזיווק 6 ,תוגוזב )תויטרוא תו תוישעמ( תיב תודובע 4-6\n",
      ":סרוקה ןויצ יביכרמ\n",
      "18% תיב תודובע\n",
      "2%– םיזיווק\n",
      "80% – תיפוס הניחב\n",
      "יפוסה ןויצה .סרוקב רבוע ןויצל יחרכה יאנת וניה יפוסה ןחבמב )הלעמו 56( רבוע ןויצ •\n",
      ". יפוסה ןחבמב ןויצל ההז היהי רבוע ןויצמ ךומנ ןויצ יפוסה ןחבמב לבקיש טנדוטס לש\n",
      ":סרוקה תורפס\n",
      "• Introduction to Algorithms (3rd edition), Cormen, Leiserson, Rivest and\n",
      "Stein. There is also an Open University Hebrew translation of half the\n",
      "textbook.\n",
      "• Open data structures, Pat Morin.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_with_pdfplumber(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        full_text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            full_text += text or \"\"  # Add text if it exists\n",
    "        return full_text\n",
    "\n",
    "# Test the function\n",
    "# file_path = \"path_to_your_pdf.pdf\"\n",
    "extracted_text = extract_text_with_pdfplumber(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/data_strcutre.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/histabrut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/compilatin.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/decsionmaker.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/mavo_miki.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/eimut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/modelim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/dc_mavo.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid table found in mavo_miki_cropped.pdf on page 2\n",
      "Processing Table 1\n",
      "Normalized Headers: ['קריאה נדרשת', 'נושא השיעור', np.float64(nan), \"מס'\"]\n",
      "Matching column found: נושא השיעור\n",
      "Error during table extraction: argument of type 'numpy.float64' is not iterable\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'(outlier  (  חריגים  זיהוי  .)data cleaning (',\n",
       " ') matplotlib ו express',\n",
       " ')Pandas (  DataFrames עבודהעם',\n",
       " '.)datamanipulation(  ניקוי נתונים  עיבוד נתונים',\n",
       " '.)intelligence',\n",
       " '.)transformation',\n",
       " '.Entity-Relationship מודל',\n",
       " '.NoSQL גישת',\n",
       " 'Regular Expressions כמוכןנלמדלעבודעם',\n",
       " 'דוגמאות ש ל פרויקטים מעשיים .',\n",
       " 'המרת data  (  נתונים  .detection',\n",
       " 'הנתונים  כולל  הרקע  ההיסטורי  והטכנולוגי .',\n",
       " 'ויזואליזציה  של  נתונים plotly   עם  (עבודה',\n",
       " 'לשפת  .SQL',\n",
       " 'מבוא  לקורס .  סקירה  כללית  של  תחום  מדעי',\n",
       " 'מחסני  נתונים business  (  עסקית  ובינה',\n",
       " 'נתונים , מאגרי  נתונים , תכנון  מסדי  נתונים',\n",
       " 'עבודה עם קבצים',\n",
       " 'רלציוניים .',\n",
       " 'שליפת  מידע  ממסדי  נתונים  רלציוניים .  מבוא'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/data_strcutre.pdf\"  # Replace with the correct path to the PDF\n",
    "# file_path1 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\"\n",
    "# pdf2 = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\"\n",
    "pdf_analiza = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/NegevNerds/syllabus_anl.pdf\"\n",
    "res6 = extract_syllabus_topic_total(\"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/cropped/mavo_miki_cropped.pdf\")\n",
    "print(len(res6))\n",
    "res6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/data_strcutre.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/histabrut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/compilatin.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/decsionmaker.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/mavo_miki.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/eimut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/modelim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/dc_mavo.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid tables in data_strcutre.pdf\n",
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AVL-Trees',\n",
       " 'Amortized analysis',\n",
       " 'B-Trees',\n",
       " 'Basic ADT’s: stacks & queues',\n",
       " 'Binary search trees',\n",
       " 'Bloom filter',\n",
       " 'Compression: Huffman, Lempel-Ziv',\n",
       " 'Data structures for disjoint sets (union find)',\n",
       " 'Elementary graph algorithms: BFS, DFS, topological Sort',\n",
       " 'Growth of functions & algorithm analysis',\n",
       " 'Hash tables',\n",
       " 'Introduction to Algorithms (3rd edition), Cormen, Leiserson, Rivest and',\n",
       " 'MST: Kruskal, Prim',\n",
       " 'Median (deterministic and randomized algorithms).',\n",
       " 'Open data structures, Pat Morin.',\n",
       " 'Priority queues (heaps).',\n",
       " 'Probability basics',\n",
       " 'Quicksort',\n",
       " 'Recurrences',\n",
       " 'Skip lists',\n",
       " 'Sorting in Linear Time'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of PDF files to check\n",
    "pdf_files = [\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/data_strcutre.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/histabrut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/compilatin.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/decsionmaker.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/mavo_miki.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/eimut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/modelim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/dc_mavo.pdf\"\n",
    "]\n",
    "\n",
    "# Check each file for tables\n",
    "# for pdf_file in pdf_files:\n",
    "#     result = has_valid_table_with_pdfplumber(pdf_file)\n",
    "#     print(f\"{os.path.basename(pdf_file)} has table: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from PyPDF2 import PdfWriter, PdfReader\n",
    "import os\n",
    "\n",
    "def crop_pdf_top_margin(pdf_path, margin_cm=4.0):\n",
    "    \"\"\"\n",
    "    Crops a specified top margin (in centimeters) from all pages of a PDF,\n",
    "    saves the result in the same directory with '_cropped' appended to the file name,\n",
    "    and returns the path to the new PDF.\n",
    "    \n",
    "    :param pdf_path: Path to the original PDF file.\n",
    "    :param margin_cm: Top margin to crop, specified in centimeters.\n",
    "    :return: Path to the cropped PDF.\n",
    "    \"\"\"\n",
    "    # Convert centimeters to points (1 cm = 28.35 points)\n",
    "    cm_to_points = margin_cm * 28.35\n",
    "\n",
    "    # Prepare output file path (same directory, _cropped appended)\n",
    "    dir_name = os.path.dirname(pdf_path)\n",
    "    base_name = os.path.basename(pdf_path).replace('.pdf', '_cropped.pdf')\n",
    "    output_path = os.path.join(dir_name, base_name)\n",
    "\n",
    "    # Initialize PDF writer\n",
    "    pdf_writer = PdfWriter()\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            width, height = page.width, page.height\n",
    "            # Crop the top margin\n",
    "            cropped_page_bbox = (0, cm_to_points, width, height)\n",
    "\n",
    "            # Use PyPDF2 to adjust page size\n",
    "            reader = PdfReader(pdf_path)\n",
    "            page_to_write = reader.pages[i]\n",
    "            page_to_write.mediabox.upper_left = (0, height - cm_to_points)\n",
    "\n",
    "            # Add the adjusted page to the writer\n",
    "            pdf_writer.add_page(page_to_write)\n",
    "\n",
    "    # Save the cropped PDF to the same directory\n",
    "    with open(output_path, \"wb\") as out_file:\n",
    "        pdf_writer.write(out_file)\n",
    "\n",
    "    print(f\"Cropped PDF saved to: {output_path}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/data_strcutre.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/bsisi_netunim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/histabrut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/compilatin.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/decsionmaker.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/mavo_miki.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/eimut.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/modelim.pdf\",\n",
    "    \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/dc_mavo.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped PDF saved to: /Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/mavo_miki_cropped.pdf\n",
      "Valid table found in mavo_miki_cropped.pdf on page 2\n",
      "Processing Table 1\n",
      "Normalized Headers: ['קריאה נדרשת', 'נושא השיעור', np.float64(nan), \"מס'\"]\n",
      "Matching column found: נושא השיעור\n",
      "Error during table extraction: argument of type 'numpy.float64' is not iterable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'(outlier  (  חריגים  זיהוי  .)data cleaning (',\n",
       " ') matplotlib ו express',\n",
       " ')Pandas (  DataFrames עבודהעם',\n",
       " '.)datamanipulation(  ניקוי נתונים  עיבוד נתונים',\n",
       " '.)intelligence',\n",
       " '.)transformation',\n",
       " '.Entity-Relationship מודל',\n",
       " '.NoSQL גישת',\n",
       " 'Regular Expressions כמוכןנלמדלעבודעם',\n",
       " 'דוגמאות ש ל פרויקטים מעשיים .',\n",
       " 'המרת data  (  נתונים  .detection',\n",
       " 'הנתונים  כולל  הרקע  ההיסטורי  והטכנולוגי .',\n",
       " 'ויזואליזציה  של  נתונים plotly   עם  (עבודה',\n",
       " 'לשפת  .SQL',\n",
       " 'מבוא  לקורס .  סקירה  כללית  של  תחום  מדעי',\n",
       " 'מחסני  נתונים business  (  עסקית  ובינה',\n",
       " 'נתונים , מאגרי  נתונים , תכנון  מסדי  נתונים',\n",
       " 'עבודה עם קבצים',\n",
       " 'רלציוניים .',\n",
       " 'שליפת  מידע  ממסדי  נתונים  רלציוניים .  מבוא'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouptdir = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/sylbus_analyzer/cropped\"\n",
    "pdf_analiza = \"/Users/davidvolodarsky/Desktop/Semeters/Semester_G/NegevNerds/NegevNerds/syllabus_anl.pdf\"\n",
    "cropt_pdf =crop_pdf_top_margin(pdf_files[5])\n",
    "res7 = extract_syllabus_topic_total(cropt_pdf)\n",
    "res7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid table found in mavo_miki_cropped.pdf on page 2\n",
      "Processing Table 1\n",
      "Normalized Headers: ['קריאה נדרשת', 'נושא השיעור', np.float64(nan), \"מס'\"]\n",
      "Matching column found: נושא השיעור\n",
      "Error during table extraction: argument of type 'numpy.float64' is not iterable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'(outlier  (  חריגים  זיהוי  .)data cleaning (',\n",
       " ') matplotlib ו express',\n",
       " ')Pandas (  DataFrames עבודהעם',\n",
       " '.)datamanipulation(  ניקוי נתונים  עיבוד נתונים',\n",
       " '.)intelligence',\n",
       " '.)transformation',\n",
       " '.Entity-Relationship מודל',\n",
       " '.NoSQL גישת',\n",
       " 'Regular Expressions כמוכןנלמדלעבודעם',\n",
       " 'דוגמאות ש ל פרויקטים מעשיים .',\n",
       " 'המרת data  (  נתונים  .detection',\n",
       " 'הנתונים  כולל  הרקע  ההיסטורי  והטכנולוגי .',\n",
       " 'ויזואליזציה  של  נתונים plotly   עם  (עבודה',\n",
       " 'לשפת  .SQL',\n",
       " 'מבוא  לקורס .  סקירה  כללית  של  תחום  מדעי',\n",
       " 'מחסני  נתונים business  (  עסקית  ובינה',\n",
       " 'נתונים , מאגרי  נתונים , תכנון  מסדי  נתונים',\n",
       " 'עבודה עם קבצים',\n",
       " 'רלציוניים .',\n",
       " 'שליפת  מידע  ממסדי  נתונים  רלציוניים .  מבוא'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res7 = extract_syllabus_topic_total(cropt_pdf)\n",
    "res7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public void dfs(int depthLimit, int minFreeMemory) {\n",
    "    boolean depthLimitReached = false;\n",
    "    int depth = 0;\n",
    "    while (true) {\n",
    "        if (checkAndResetBacktrackRequest() || !isNewState() || isEndState() ||\n",
    "            isIgnoredState() || depthLimitReached) {\n",
    "            if (!backtrack()) { // backtrack not possible, done\n",
    "            return;\n",
    "            }\n",
    "            depthLimitReached = \n",
    "                false;\n",
    "            depth--;\n",
    "            notifyStateBacktracked();\n",
    "            } if (\n",
    "                forward()) {\n",
    "                depth++;\n",
    "                notifyStateAdvanced();\n",
    "                if (currentError != null) {\n",
    "                    notifyPropertyViolated();\n",
    "                    if (hasPropertyTermination()) {\n",
    "                    return;\n",
    "                    }\n",
    "                } if (\n",
    "                    depth >= depthLimit) {\n",
    "                    depthLimitReached = true;\n",
    "                    notifySearchConstraintHit(\"depth limit reached: \" + depthLimit);\n",
    "                    continue;\n",
    "                } if (!checkStateSpaceLimit(minFreeMemory)) {\n",
    "                    notifySearchConstraintHit(\"memory limit reached: \" + minFreeMemory);\n",
    "                    return;\n",
    "                }\n",
    "                } else { // forward did not execute any instructions\n",
    "                notifyStateProcessed();\n",
    "                }\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
